{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dk9dnygJt_nR"
      },
      "source": [
        "# **Week 14: GNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8i13DQduMjT"
      },
      "source": [
        "Today class consists of three things. <br>\n",
        "\n",
        "## **1.  We will Make Graph Convolution Equation.**\n",
        "1-1. We will make graph by using networx libary. <br> \n",
        "1-2. by using Adjacency Matrix, Node index and Node embedding vector from graph, We will follow the aggregation and combination step in Graph Convolution Equation. <br>\n",
        "1-3. Finally We will make GCN layer <br>\n",
        "\n",
        "## **2.  We will make node classification in Cora dataset.**\n",
        "2-1. Cora dataset Information <br>\n",
        "2-2. Implement GCN model with Cora dataset <br>\n",
        "2-3. Visualize node embedding\n",
        "\n",
        "## **3.  (HOMEWORK) Run the Graph classification on the Collab Dataset**\n",
        "3-1. I will introduce some brief information about the code and pytorch geometric.  <br><br>\n",
        "\n",
        "If you have any questions, feel free to ask\n",
        "\n",
        "*   E-Mail Address : seongjunyang@kaist.ac.kr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZ52uzalrH8T"
      },
      "source": [
        "## **Prelims** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wG1F_2xo0IP"
      },
      "source": [
        "pip install --user ipdb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "pip install --user decorator==4.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "pip install --user networkx==2.6.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "awxC1FIj3JKb"
      },
      "outputs": [],
      "source": [
        "import ipdb\n",
        "import torch\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.linalg import fractional_matrix_power\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-awmjVA27QF"
      },
      "source": [
        "### **1. Make Graph Convolution Equation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exwpV6zl2Bjh"
      },
      "source": [
        "![picture](https://drive.google.com/uc?id=1BqgZ_3xUQ7ScvoPVHUZbx5o4xBGSlLVS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CslE1Iny38se"
      },
      "source": [
        "#### **1) Initialize the Graph G**\n",
        "By using networkx library, you can do research in graph or network easily. <br>\n",
        "So, in the Graph Convolution Equation, I'll use networkx libary. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "c7pc-dYFLFxj"
      },
      "outputs": [],
      "source": [
        "#1. Initialize the graph\n",
        "G = nx.Graph(name='G')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zhFhYZKrLIZw"
      },
      "outputs": [],
      "source": [
        "#2. Create nodes\n",
        "#In this class, we will make graph that consist of 6 nodes.\n",
        "#Each node is assigned node feature which corresponds to the node name\n",
        "for i in range(1,7):\n",
        "    G.add_node(i, name=i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KA5OEy53LMB8"
      },
      "outputs": [],
      "source": [
        "#Define the edges and the edges to the graph\n",
        "edges = [(1,2), (1,3), (2,4), (2,5), (3,4), (3,6) ]\n",
        "G.add_edges_from(edges)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CywDujBwLOGa",
        "outputId": "8cc40285-fccd-4073-afc7-85e7ff0c56b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Graph Info:\n",
            " Name: G\n",
            "Type: Graph\n",
            "Number of nodes: 6\n",
            "Number of edges: 6\n",
            "Average degree:   2.0000\n"
          ]
        }
      ],
      "source": [
        "#See graph info\n",
        "print('Graph Info:\\n', nx.info(G))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52RnpoFbLWi8",
        "outputId": "7d635efc-56b8-4c3b-dc7a-ab5d319416b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Graph Nodes:  [(1, {'name': 1}), (2, {'name': 2}), (3, {'name': 3}), (4, {'name': 4}), (5, {'name': 5}), (6, {'name': 6})]\n"
          ]
        }
      ],
      "source": [
        "#Inspect the node features\n",
        "print('\\nGraph Nodes: ', G.nodes.data())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-bCgeY44HRH"
      },
      "source": [
        "#### Inserting Adjacency Matrix to forward pass "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDIQRBxAsDbQ",
        "outputId": "0b9a3339-39f3-4f58-9a6a-fcd3de17be60"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(matrix([[0., 1., 1., 0., 0., 0.],\n",
              "         [1., 0., 0., 1., 1., 0.],\n",
              "         [1., 0., 0., 1., 0., 1.],\n",
              "         [0., 1., 1., 0., 0., 0.],\n",
              "         [0., 1., 0., 0., 0., 0.],\n",
              "         [0., 0., 1., 0., 0., 0.]]),\n",
              " [1, 2, 3, 4, 5, 6])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nx.attr_matrix(G, node_attr='name')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zpwgjWUj3BKj"
      },
      "outputs": [],
      "source": [
        "#Get the Adjacency Matrix (A) and Node Features Matrix (X) as numpy array\n",
        "A = np.array(nx.attr_matrix(G, node_attr='name')[0]) # Converting for getting numpy Adjacency Matrix (A)\n",
        "X = np.array(nx.attr_matrix(G, node_attr='name')[1]) # Converting for getting numpy Node Features Matrix (X)\n",
        "X = np.expand_dims(X,axis=1) # Make [6, 1] numpy Node Features Matrix (X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPosg1IkLjcz",
        "outputId": "fd87a5c5-b5b1-4a86-c3b9-9975d0643120"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of A:  (6, 6)\n"
          ]
        }
      ],
      "source": [
        "print('Shape of A: ', A.shape) # [6, 6] matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40OvPaEGLm7v",
        "outputId": "5bd0e560-a63a-48c2-f6d0-e6e1ae3030d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Shape of X:  (6, 1)\n"
          ]
        }
      ],
      "source": [
        "print('\\nShape of X: ', X.shape) # [6, 1] matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCvbW_emLouR",
        "outputId": "a759c232-fc0e-43c8-e1e3-8815719f6881"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Adjacency Matrix (A):\n",
            " [[0. 1. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 1. 1. 0.]\n",
            " [1. 0. 0. 1. 0. 1.]\n",
            " [0. 1. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "print('\\nAdjacency Matrix (A):\\n', A)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UO5FhvnNLqzB",
        "outputId": "d6e0e2f2-2d17-417b-c7a8-6ace21ac0d7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Node Features Matrix (X):\n",
            " [[1]\n",
            " [2]\n",
            " [3]\n",
            " [4]\n",
            " [5]\n",
            " [6]]\n"
          ]
        }
      ],
      "source": [
        "print('\\nNode Features Matrix (X):\\n', X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Be1Zv86g3BQJ",
        "outputId": "38935ef8-c5e1-4cdf-c908-cc64af114022"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dot product of A and X (AX):\n",
            " [[ 5.]\n",
            " [10.]\n",
            " [11.]\n",
            " [ 5.]\n",
            " [ 2.]\n",
            " [ 3.]]\n"
          ]
        }
      ],
      "source": [
        "#Dot product Adjacency Matrix (A) and Node Features (X)\n",
        "AX = np.dot(A,X) # AX\n",
        "print(\"Dot product of A and X (AX):\\n\", AX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Txfh9ecN8WN5"
      },
      "source": [
        "##### **Question : Is this the node representations H?**\n",
        "##### No, A*X is just neighbor aggregation.\n",
        "##### **We need the combination step!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0s_dR2n4m0f"
      },
      "source": [
        "#### **Add Self-Loops and Normalize Adjacency Matrix (A)**\n",
        "**A' = A + I**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ELrUjRjT3Bei"
      },
      "outputs": [],
      "source": [
        "#Add Self Loops\n",
        "G_self_loops = G.copy() # A'\n",
        "\n",
        "self_loops = []\n",
        "for i in range(1, 1+ G.number_of_nodes()):\n",
        "    self_loops.append((i,i))\n",
        "\n",
        "G_self_loops.add_edges_from(self_loops) # A' = A + I\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxgUsPpEL935",
        "outputId": "f28535e9-d77e-4961-9ba0-3cad0328fb87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Edges of G with self-loops:\n",
            " [(1, 2), (1, 3), (1, 1), (2, 4), (2, 5), (2, 2), (3, 4), (3, 6), (3, 3), (4, 4), (5, 5), (6, 6)]\n"
          ]
        }
      ],
      "source": [
        "#Check the edges of G_self_loops after adding the self loops\n",
        "print('Edges of G with self-loops:\\n', G_self_loops.edges)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zkxe3s67MANc",
        "outputId": "ff8ae4a3-2a03-4c5b-e00f-bd6d0154f54b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adjacency Matrix of added self-loops G (A_hat):\n",
            " [[1. 1. 1. 0. 0. 0.]\n",
            " [1. 1. 0. 1. 1. 0.]\n",
            " [1. 0. 1. 1. 0. 1.]\n",
            " [0. 1. 1. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 1. 0.]\n",
            " [0. 0. 1. 0. 0. 1.]]\n"
          ]
        }
      ],
      "source": [
        "#Get the Adjacency Matrix (A) and Node Features Matrix (X) of added self-lopps graph\n",
        "A_hat = np.array(nx.attr_matrix(G_self_loops, node_attr='name')[0]) # A' numpy Matrix\n",
        "print('Adjacency Matrix of added self-loops G (A_hat):\\n', A_hat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEnlzJwK9yYX"
      },
      "source": [
        "##### **A' * X Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crADwAUBMCGo",
        "outputId": "981019b0-e3c9-4e5e-9da7-b21df3b46bdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A_hatX:\n",
            " [[ 6.]\n",
            " [12.]\n",
            " [14.]\n",
            " [ 9.]\n",
            " [ 7.]\n",
            " [ 9.]]\n"
          ]
        }
      ],
      "source": [
        "#Calculate the dot product of A_hat and X (AX)\n",
        "A_hatX = np.dot(A_hat, X)\n",
        "print('A_hatX:\\n', A_hatX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQYRTZ5m-HE8"
      },
      "source": [
        "##### But, there is another problem.\n",
        "##### Scales of node features differ by the number of neighbors.\n",
        "##### Solution : Normalization by inverse degree matrix.\n",
        "##### **D_inverse * A'**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Es8HJ5g9J5Gk",
        "outputId": "e67649a8-159f-4638-afbf-ef2c16b4f8c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1, 3], [2, 4], [3, 4], [4, 3], [5, 2], [6, 2]]\n"
          ]
        }
      ],
      "source": [
        "#Get the Degree Matrix of the added self-loops graph # 몇개씩 연결되어있는지 연산\n",
        "edge_List = G_self_loops.edges() \n",
        "Deg_Mat = [[i, 0] for i in G_self_loops.nodes()]\n",
        "\n",
        "for element in edge_List:\n",
        "  if element[0] != element[1]:\n",
        "    Deg_Mat[element[0] - 1][1] += 1\n",
        "    Deg_Mat[element[1] - 1][1] += 1\n",
        "  else :\n",
        "    Deg_Mat[element[0] - 1][1] += 1\n",
        "\n",
        "print(Deg_Mat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "EdgeView([(1, 2), (1, 3), (1, 1), (2, 4), (2, 5), (2, 2), (3, 4), (3, 6), (3, 3), (4, 4), (5, 5), (6, 6)])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "edge_List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "NodeView((1, 2, 3, 4, 5, 6))"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "G_self_loops.nodes()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[1, 3], [2, 4], [3, 4], [4, 3], [5, 2], [6, 2]]"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Deg_Mat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MV6cc05jMHx1",
        "outputId": "69e52c42-b3cc-4121-b884-9afe408f6a06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Degree Matrix of added self-loops G as numpy array (D):\n",
            " [[3 0 0 0 0 0]\n",
            " [0 4 0 0 0 0]\n",
            " [0 0 4 0 0 0]\n",
            " [0 0 0 3 0 0]\n",
            " [0 0 0 0 2 0]\n",
            " [0 0 0 0 0 2]]\n"
          ]
        }
      ],
      "source": [
        "#Convert the Degree Matrix to a N x N matrix where N is the number of nodes\n",
        "D = np.diag([deg for [n,deg] in Deg_Mat]) # Get degree matrix\n",
        "\n",
        "print('Degree Matrix of added self-loops G as numpy array (D):\\n', D)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eROSLv3__ygH"
      },
      "source": [
        "##### **D_inverse**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dg_13neAMJ12",
        "outputId": "58f88b84-f095-433b-ff37-979f18bc605c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inverse of D:\n",
            " [[ 0.33333333  0.          0.          0.          0.          0.        ]\n",
            " [ 0.          0.25        0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.25        0.          0.          0.        ]\n",
            " [-0.         -0.         -0.          0.33333333 -0.         -0.        ]\n",
            " [ 0.          0.          0.          0.          0.5         0.        ]\n",
            " [ 0.          0.          0.          0.          0.          0.5       ]]\n"
          ]
        }
      ],
      "source": [
        "#Get the inverse of Degree Matrix (D)\n",
        "D_inv = np.linalg.inv(D)\n",
        "print('Inverse of D:\\n', D_inv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oq-IWob0AQEb"
      },
      "source": [
        "##### **D_inverse*A'**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAv47XgDAQnM",
        "outputId": "3467c55f-e41d-4e80-e9b8-0aa3a481af95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.33333333 0.33333333 0.33333333 0.         0.         0.        ]\n",
            " [0.25       0.25       0.         0.25       0.25       0.        ]\n",
            " [0.25       0.         0.25       0.25       0.         0.25      ]\n",
            " [0.         0.33333333 0.33333333 0.33333333 0.         0.        ]\n",
            " [0.         0.5        0.         0.         0.5        0.        ]\n",
            " [0.         0.         0.5        0.         0.         0.5       ]]\n"
          ]
        }
      ],
      "source": [
        "D_invA = np.dot(D_inv, A_hat)\n",
        "print(D_invA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zu1nCQ3AC82"
      },
      "source": [
        "##### **D_invA'X**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMPl4wN0432i",
        "outputId": "05bf664f-ff1c-45e3-bcf3-52d90ac5fb36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DAXW:\n",
            " [[2. ]\n",
            " [3. ]\n",
            " [3.5]\n",
            " [3. ]\n",
            " [3.5]\n",
            " [4.5]]\n"
          ]
        }
      ],
      "source": [
        "#Dot product of D and AX for normalization\n",
        "DAX = np.dot(D_invA,X)\n",
        "\n",
        "print('DAXW:\\n', DAX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt2m1K3s5F7L"
      },
      "source": [
        "#### **Add Weights and Activation Function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qlTPzgG3Bqj",
        "outputId": "b6be7e70-8070-40e4-9830-828bdc007364"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W0 weight:\n",
            " [[-0.00204708  0.00478943 -0.00519439 -0.0055573 ]]\n",
            "W1 weight:\n",
            " [[ 0.01965781  0.01393406]\n",
            " [ 0.00092908  0.00281746]\n",
            " [ 0.00769023  0.01246435]\n",
            " [ 0.01007189 -0.01296221]]\n"
          ]
        }
      ],
      "source": [
        "#Initialize the weights\n",
        "np.random.seed(12345)\n",
        "n_h = 4 #number of neurons in the hidden layer\n",
        "n_y = 2 #number of neurons in the output layer\n",
        "W0 = np.random.randn(X.shape[1],n_h) * 0.01\n",
        "W1 = np.random.randn(n_h,n_y) * 0.01\n",
        "\n",
        "print('W0 weight:\\n', W0)\n",
        "print('W1 weight:\\n', W1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMD2Yhali3ZY"
      },
      "source": [
        "#### **GCN Layer**\n",
        "##### **TODO : fill ????? with proper code and run**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Y-db5ci2MsuE"
      },
      "outputs": [],
      "source": [
        "#Implement ReLu as activation function,\n",
        "#Originally, non-linear activation needed, but when I searched some material, relu is used for activate function generally.\n",
        "def relu(x):\n",
        "    return np.maximum(0,x)\n",
        "\n",
        "#Build GCN layer\n",
        "#In this function, we implement numpy to simplify\n",
        "def gcn(A,H,W):\n",
        "    #ipdb.set_trace()\n",
        "    I = np.identity(A.shape[0])\n",
        "    A_hat = A + I\n",
        "    D = np.diag(np.sum(A_hat, axis = 0))\n",
        "    D_inv = np.linalg.inv(D)\n",
        "    D_invA = np.dot(D_inv, A_hat)\n",
        "    DAXW = np.dot(D_invA,H).dot(W)\n",
        "    return relu(DAXW)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYO2IsTStFXp",
        "outputId": "da854206-d7c9-4f67-bc1f-44ba123a14a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Node Embedding from GCN output:\n",
            " [[1.26076558e-05 3.82331255e-05]\n",
            " [1.27930625e-05 3.87953773e-05]\n",
            " [1.44617228e-05 4.38556439e-05]\n",
            " [1.40909094e-05 4.27311403e-05]\n",
            " [1.44617228e-05 4.38556439e-05]\n",
            " [1.77990434e-05 5.39761772e-05]]\n"
          ]
        }
      ],
      "source": [
        "#Do forward propagation\n",
        "H1 = gcn(A,X,W0) # Make GCN layer\n",
        "H2 = gcn(A,H1,W1) # Make GCN layer\n",
        "print('Node Embedding from GCN output:\\n', H2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb_6f2E25MwO"
      },
      "source": [
        "#### **Plotting Node Embedding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "_4GiB6AO5R-z",
        "outputId": "8c4575ff-2072-4d23-c53c-8a065428eec5"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "def visualize(h, color):\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.xlim([np.min(h[:,0])*0.9, np.max(h[:,0])*1.1])\n",
        "    plt.xlabel('Dimension 0')\n",
        "    plt.ylabel('Dimension 1')\n",
        "\n",
        "    plt.scatter(h[:, 0], h[:, 1], s=140, c=color, cmap=\"Set2\")\n",
        "    plt.show()\n",
        "\n",
        "visualize(H2, color=range(6)) # node3 and node 5 have same embedding, So Two nodes overlap on the screen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gz7DGjMzWIMk"
      },
      "source": [
        "## **2. Node classification on Cora Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NWlhYrOiV7P"
      },
      "source": [
        "### **Prelims**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "LipkMKfEWGma"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "import torch.optim as optim\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3uSXXkVmB_5"
      },
      "source": [
        "### **Cora Dataset**\n",
        "Dataset link : https://relational.fit.cvut.cz/dataset/CORA <br>\n",
        "The Cora dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words. <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOaW-DOKkYU6"
      },
      "source": [
        "!wget https://www.dropbox.com/s/fl9mvrio3hah4on/cora.content\n",
        "!wget https://www.dropbox.com/s/l829sldp7xqrt0h/cora.cites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "C-IItR-W48eT",
        "outputId": "224a9399-23d3-4e5e-e149-712df2d0ce30"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>source</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4294</th>\n",
              "      <td>152731</td>\n",
              "      <td>1109392</td>\n",
              "      <td>cites</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2625</th>\n",
              "      <td>28385</td>\n",
              "      <td>118558</td>\n",
              "      <td>cites</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4255</th>\n",
              "      <td>144408</td>\n",
              "      <td>219446</td>\n",
              "      <td>cites</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3696</th>\n",
              "      <td>78555</td>\n",
              "      <td>78557</td>\n",
              "      <td>cites</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>437</th>\n",
              "      <td>1365</td>\n",
              "      <td>188318</td>\n",
              "      <td>cites</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      target   source  label\n",
              "4294  152731  1109392  cites\n",
              "2625   28385   118558  cites\n",
              "4255  144408   219446  cites\n",
              "3696   78555    78557  cites\n",
              "437     1365   188318  cites"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "edgelist = pd.read_csv(\"../../../data/ai504/cora.cites\", sep='\\t', header=None, names=[\"target\", \"source\"]) # it has graph\n",
        "edgelist[\"label\"] = \"cites\"\n",
        "edgelist.sample(frac=1).head(5) # <ID of cited paper node> <ID of citing paper node>, by doing this, you can see the edge information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acDPH5FT6Jht",
        "outputId": "556acc27-61a4-4aad-e2a9-1789163d34c5"
      },
      "outputs": [],
      "source": [
        "# 노드 index list를 가져올 수 있게 됨\n",
        "Gnx = nx.from_pandas_edgelist(edgelist, edge_attr=\"label\")\n",
        "nx.set_node_attributes(Gnx, \"paper\", \"label\")\n",
        "\n",
        "# print(Gnx.nodes) # from edgelist, by using from_pandas_edgelist() function, we can extract node list from edgelist\n",
        "# Gnx.nodes[12210] ## by type this, we can see the node feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "1RrvIYLd8iE6",
        "outputId": "c581152c-3ea9-406c-8be3-5d6aa49cc303"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_0</th>\n",
              "      <th>word_1</th>\n",
              "      <th>word_2</th>\n",
              "      <th>word_3</th>\n",
              "      <th>word_4</th>\n",
              "      <th>word_5</th>\n",
              "      <th>word_6</th>\n",
              "      <th>word_7</th>\n",
              "      <th>word_8</th>\n",
              "      <th>word_9</th>\n",
              "      <th>...</th>\n",
              "      <th>word_1424</th>\n",
              "      <th>word_1425</th>\n",
              "      <th>word_1426</th>\n",
              "      <th>word_1427</th>\n",
              "      <th>word_1428</th>\n",
              "      <th>word_1429</th>\n",
              "      <th>word_1430</th>\n",
              "      <th>word_1431</th>\n",
              "      <th>word_1432</th>\n",
              "      <th>subject</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>31336</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Neural_Networks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1061127</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Rule_Learning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1106406</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Reinforcement_Learning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13195</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Reinforcement_Learning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37879</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Probabilistic_Methods</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1434 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         word_0  word_1  word_2  word_3  word_4  word_5  word_6  word_7  \\\n",
              "31336         0       0       0       0       0       0       0       0   \n",
              "1061127       0       0       0       0       0       0       0       0   \n",
              "1106406       0       0       0       0       0       0       0       0   \n",
              "13195         0       0       0       0       0       0       0       0   \n",
              "37879         0       0       0       0       0       0       0       0   \n",
              "\n",
              "         word_8  word_9  ...  word_1424  word_1425  word_1426  word_1427  \\\n",
              "31336         0       0  ...          0          0          1          0   \n",
              "1061127       0       0  ...          0          1          0          0   \n",
              "1106406       0       0  ...          0          0          0          0   \n",
              "13195         0       0  ...          0          0          0          0   \n",
              "37879         0       0  ...          0          0          0          0   \n",
              "\n",
              "         word_1428  word_1429  word_1430  word_1431  word_1432  \\\n",
              "31336            0          0          0          0          0   \n",
              "1061127          0          0          0          0          0   \n",
              "1106406          0          0          0          0          0   \n",
              "13195            0          0          0          0          0   \n",
              "37879            0          0          0          0          0   \n",
              "\n",
              "                        subject  \n",
              "31336           Neural_Networks  \n",
              "1061127           Rule_Learning  \n",
              "1106406  Reinforcement_Learning  \n",
              "13195    Reinforcement_Learning  \n",
              "37879     Probabilistic_Methods  \n",
              "\n",
              "[5 rows x 1434 columns]"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "feature_names = [\"word_{}\".format(ii) for ii in range(1433)]\n",
        "column_names =  feature_names + [\"subject\"]\n",
        "node_data = pd.read_csv(\"../../../data/ai504/cora.content\", sep='\\t', header=None, names=column_names)\n",
        "node_data.head(5) # <paper node id> <word_attributes>+ <node label>\n",
        "\n",
        "# 여기에는 node index별로 node occurancy table을 가져올 수 있음"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOzSKBzM8zIO",
        "outputId": "53068a60-9eb2-4333-9aef-80572da3e966"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Case_Based',\n",
              " 'Genetic_Algorithms',\n",
              " 'Neural_Networks',\n",
              " 'Probabilistic_Methods',\n",
              " 'Reinforcement_Learning',\n",
              " 'Rule_Learning',\n",
              " 'Theory'}"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 7가지 subject 에 대한 정보로, target 임\n",
        "set(node_data[\"subject\"]) # node class type"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiudoKBPBdTM"
      },
      "source": [
        "In the class, we will predict the subject of a paper (node) on the basis of the surrounding node data and the structure of the graph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6HYsYyKmG5B"
      },
      "source": [
        "### **Hyperparameter**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "6dJCt03KbJag"
      },
      "outputs": [],
      "source": [
        "EPOCH = 200\n",
        "SEED = 42\n",
        "NUM_HIDDEN = 16\n",
        "dropout_rate = 0.5\n",
        "learning_rate = 0.01\n",
        "weight_decay = 5e-4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3-22nfIjGgU"
      },
      "source": [
        "### **Preprocess and Make Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "7hy0APqxYSfQ"
      },
      "outputs": [],
      "source": [
        "# Class를 One-hot으로 바꿈\n",
        "def encode_onehot(labels): # we will make all class(subject) to one-hot vector for training.\n",
        "    classes = set(labels) # {'Case_Based', 'Genetic_Algorithms', 'Neural_Networks', 'Probabilistic_Methods', 'Reinforcement_Learning', 'Rule_Learning', 'Theory'}\n",
        "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
        "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
        "    return labels_onehot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "yubfrdUVZu85"
      },
      "outputs": [],
      "source": [
        "def normalize(mx): # This part is similar to the normalization process implemented earlier.\n",
        "    #ipdb.set_trace()\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "8KbsLOMDZ4HT"
      },
      "outputs": [],
      "source": [
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx): # Convert a scipy sparse matrix to a torch sparse tensor.\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    return torch.sparse.FloatTensor(indices, values, shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "3irNhvUvZGXA"
      },
      "outputs": [],
      "source": [
        "def load_data(path=\"../../../data/ai504/\", dataset=\"cora\"):\n",
        "    # In the function, by using above 3 function, \n",
        "    print('Loading {} dataset...'.format(dataset))\n",
        "\n",
        "    #ipdb.set_trace()\n",
        "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset), dtype=np.dtype(str)) # load all tables\n",
        "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)  # Compress sparse matrix\n",
        "    labels = encode_onehot(idx_features_labels[:, -1]) # Label onehot encoding\n",
        "\n",
        "    # build graph\n",
        "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32) # node list\n",
        "    idx_map = {j: i for i, j in enumerate(idx)}\n",
        "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),dtype=np.int32)\n",
        "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
        "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])), shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)\n",
        "\n",
        "    # build adjacency matrix\n",
        "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "\n",
        "    features = normalize(features)\n",
        "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
        "\n",
        "    # split all nodes to train/valid/test for node classification\n",
        "    idx_train = range(140)\n",
        "    idx_val = range(200, 500)\n",
        "    idx_test = range(500, 1500)\n",
        "\n",
        "    features = torch.FloatTensor(np.array(features.todense()))\n",
        "    labels = torch.LongTensor(np.where(labels)[1])\n",
        "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
        "\n",
        "    idx_train = torch.LongTensor(idx_train)\n",
        "    idx_val = torch.LongTensor(idx_val)\n",
        "    idx_test = torch.LongTensor(idx_test)\n",
        "\n",
        "    return adj, features, labels, idx_train, idx_val, idx_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "peM32LKqZGbQ"
      },
      "outputs": [],
      "source": [
        "def accuracy(output, labels):\n",
        "    preds = output.max(1)[1].type_as(labels)\n",
        "    correct = preds.eq(labels).double()\n",
        "    correct = correct.sum()\n",
        "    return correct / len(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDvEaIQHh7Tj"
      },
      "source": [
        "#### **Model Architecture**\n",
        "##### **TODO : Fill ????? with proper code and Run**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "0DkYZGo7W8Ka"
      },
      "outputs": [],
      "source": [
        "class GraphConvolution(Module):\n",
        "    \n",
        "    #Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
        "    \n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        # initialize weight by using reset_parameters() function\n",
        "        self.in_features = in_features \n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        support = torch.mm(input, self.weight)\n",
        "        output = torch.mm(adj, support)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "dCt68pu3XfIT"
      },
      "outputs": [],
      "source": [
        "class GCN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc2 = GraphConvolution(nhid, nclass)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "      # Obtain Node embedding\n",
        "      #ipdb.set_trace()\n",
        "      # Make forward propagation by referencing Section 1 (Graph Convolution Equation's forward propagation).\n",
        "\n",
        "      x = self.gc1(x, adj)\n",
        "      x = F.relu(x)\n",
        "      x = F.dropout(x, self.dropout, training= self.training)\n",
        "      x = self.gc2(x,adj)\n",
        "      x = F.log_softmax(x, dim = 1)\n",
        "      return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk1563rIhuiJ"
      },
      "source": [
        "#### **Setting for training model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "RJw6icvtXfhx"
      },
      "outputs": [],
      "source": [
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3grxOsbXfj6",
        "outputId": "da27fed4-5146-4c6f-a76a-7d6a1275abbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading cora dataset...\n",
            "Wall time: 3.84 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Load data\n",
        "adj, features, labels, idx_train, idx_val, idx_test = load_data() # adj -> adjacency matrix, same ax A,   features -> node feature matrix, same as X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "Ky5HcyaFXfmR"
      },
      "outputs": [],
      "source": [
        "# Model and optimizer\n",
        "model = GCN(nfeat=features.shape[1], # [2708, 1433] -> [1433] for matrix multiplication of X and W\n",
        "            nhid=NUM_HIDDEN,\n",
        "            nclass=labels.max().item() + 1,\n",
        "            dropout=dropout_rate)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "sA9MOwNJW8Qq"
      },
      "outputs": [],
      "source": [
        "model.cuda()\n",
        "features = features.cuda()\n",
        "adj = adj.cuda()\n",
        "labels = labels.cuda()\n",
        "idx_train = idx_train.cuda()\n",
        "idx_val = idx_val.cuda()\n",
        "idx_test = idx_test.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJq8sLZpaKlZ"
      },
      "source": [
        "### **Train code**\n",
        "In the train() function, We train GCN by using nll_loss objective function and Adam Optimizer. <br>\n",
        "By using train and validation index, We get output in model result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "YT9hJiCnalxc"
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "    t = time.time()\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output = model(features, adj)\n",
        "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
        "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Evaluate validation set performance separately,\n",
        "    # deactivates dropout during validation run.\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "\n",
        "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
        "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
        "\n",
        "    print('Epoch: {:04d}'.format(epoch+1),\n",
        "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
        "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "          'acc_val: {:.4f}'.format(acc_val.item()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPb0nqIWbI7_"
      },
      "source": [
        "### **Test code**\n",
        "In the test() function, we test trained model with node embedding visualization (T-SNE)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GCN(\n",
              "  (gc1): GraphConvolution()\n",
              "  (gc2): GraphConvolution()\n",
              ")"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "_lSvl3yLaqXb"
      },
      "outputs": [],
      "source": [
        "# Visualize\n",
        "def visualize(h, label, idx):\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.xlabel('Dimension 0')\n",
        "    plt.ylabel('Dimension 1')\n",
        "\n",
        "    h_ = h[idx]\n",
        "    color = [ label[i] for i in idx ]\n",
        "    print(f'Embedding shape: {list(h_.shape)}')\n",
        "    z = TSNE(n_components=2).fit_transform(h_.cpu().detach().numpy())  \n",
        "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
        "    plt.show()\n",
        "\n",
        "def test(): # get loss and accuracy with node embedding visualization\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "    visualize(output, labels, idx_test)\n",
        "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
        "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.item()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-PxdS5tbf9q"
      },
      "source": [
        "### **Train**\n",
        "When I measure time for traing, About 1.35 sec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2uHuNHXasOm",
        "outputId": "3709454f-61f9-400c-a5fe-1ca5f043eab6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0001 loss_train: 0.3813 acc_train: 0.9500 loss_val: 0.7097 acc_val: 0.8067\n",
            "Epoch: 0002 loss_train: 0.3507 acc_train: 0.9571 loss_val: 0.7082 acc_val: 0.8067\n",
            "Epoch: 0003 loss_train: 0.3458 acc_train: 0.9429 loss_val: 0.7067 acc_val: 0.8133\n",
            "Epoch: 0004 loss_train: 0.3947 acc_train: 0.9500 loss_val: 0.7057 acc_val: 0.8133\n",
            "Epoch: 0005 loss_train: 0.3766 acc_train: 0.9429 loss_val: 0.7049 acc_val: 0.8133\n",
            "Epoch: 0006 loss_train: 0.3471 acc_train: 0.9571 loss_val: 0.7045 acc_val: 0.8133\n",
            "Epoch: 0007 loss_train: 0.3804 acc_train: 0.9429 loss_val: 0.7032 acc_val: 0.8133\n",
            "Epoch: 0008 loss_train: 0.3766 acc_train: 0.9429 loss_val: 0.7018 acc_val: 0.8133\n",
            "Epoch: 0009 loss_train: 0.3657 acc_train: 0.9571 loss_val: 0.7003 acc_val: 0.8133\n",
            "Epoch: 0010 loss_train: 0.3789 acc_train: 0.9571 loss_val: 0.6982 acc_val: 0.8100\n",
            "Epoch: 0011 loss_train: 0.3795 acc_train: 0.9429 loss_val: 0.6964 acc_val: 0.8100\n",
            "Epoch: 0012 loss_train: 0.3521 acc_train: 0.9643 loss_val: 0.6949 acc_val: 0.8100\n",
            "Epoch: 0013 loss_train: 0.3467 acc_train: 0.9429 loss_val: 0.6938 acc_val: 0.8067\n",
            "Epoch: 0014 loss_train: 0.3514 acc_train: 0.9357 loss_val: 0.6927 acc_val: 0.8067\n",
            "Epoch: 0015 loss_train: 0.3603 acc_train: 0.9571 loss_val: 0.6913 acc_val: 0.8067\n",
            "Epoch: 0016 loss_train: 0.3705 acc_train: 0.9357 loss_val: 0.6899 acc_val: 0.8133\n",
            "Epoch: 0017 loss_train: 0.3763 acc_train: 0.9429 loss_val: 0.6891 acc_val: 0.8167\n",
            "Epoch: 0018 loss_train: 0.3097 acc_train: 0.9571 loss_val: 0.6886 acc_val: 0.8200\n",
            "Epoch: 0019 loss_train: 0.3429 acc_train: 0.9429 loss_val: 0.6877 acc_val: 0.8233\n",
            "Epoch: 0020 loss_train: 0.3505 acc_train: 0.9500 loss_val: 0.6870 acc_val: 0.8233\n",
            "Epoch: 0021 loss_train: 0.3423 acc_train: 0.9429 loss_val: 0.6867 acc_val: 0.8233\n",
            "Epoch: 0022 loss_train: 0.3668 acc_train: 0.9429 loss_val: 0.6865 acc_val: 0.8233\n",
            "Epoch: 0023 loss_train: 0.3381 acc_train: 0.9500 loss_val: 0.6862 acc_val: 0.8267\n",
            "Epoch: 0024 loss_train: 0.3535 acc_train: 0.9714 loss_val: 0.6854 acc_val: 0.8267\n",
            "Epoch: 0025 loss_train: 0.3335 acc_train: 0.9571 loss_val: 0.6846 acc_val: 0.8267\n",
            "Epoch: 0026 loss_train: 0.3430 acc_train: 0.9429 loss_val: 0.6842 acc_val: 0.8267\n",
            "Epoch: 0027 loss_train: 0.3407 acc_train: 0.9571 loss_val: 0.6840 acc_val: 0.8267\n",
            "Epoch: 0028 loss_train: 0.3291 acc_train: 0.9571 loss_val: 0.6837 acc_val: 0.8267\n",
            "Epoch: 0029 loss_train: 0.3269 acc_train: 0.9571 loss_val: 0.6834 acc_val: 0.8267\n",
            "Epoch: 0030 loss_train: 0.3402 acc_train: 0.9643 loss_val: 0.6835 acc_val: 0.8267\n",
            "Epoch: 0031 loss_train: 0.3700 acc_train: 0.9357 loss_val: 0.6842 acc_val: 0.8267\n",
            "Epoch: 0032 loss_train: 0.3292 acc_train: 0.9500 loss_val: 0.6845 acc_val: 0.8233\n",
            "Epoch: 0033 loss_train: 0.3371 acc_train: 0.9500 loss_val: 0.6843 acc_val: 0.8233\n",
            "Epoch: 0034 loss_train: 0.2981 acc_train: 0.9786 loss_val: 0.6843 acc_val: 0.8267\n",
            "Epoch: 0035 loss_train: 0.3571 acc_train: 0.9714 loss_val: 0.6842 acc_val: 0.8267\n",
            "Epoch: 0036 loss_train: 0.3205 acc_train: 0.9500 loss_val: 0.6843 acc_val: 0.8267\n",
            "Epoch: 0037 loss_train: 0.3563 acc_train: 0.9500 loss_val: 0.6838 acc_val: 0.8267\n",
            "Epoch: 0038 loss_train: 0.3330 acc_train: 0.9643 loss_val: 0.6838 acc_val: 0.8267\n",
            "Epoch: 0039 loss_train: 0.3490 acc_train: 0.9643 loss_val: 0.6832 acc_val: 0.8267\n",
            "Epoch: 0040 loss_train: 0.3108 acc_train: 0.9857 loss_val: 0.6820 acc_val: 0.8267\n",
            "Epoch: 0041 loss_train: 0.3292 acc_train: 0.9857 loss_val: 0.6806 acc_val: 0.8267\n",
            "Epoch: 0042 loss_train: 0.3105 acc_train: 0.9714 loss_val: 0.6790 acc_val: 0.8267\n",
            "Epoch: 0043 loss_train: 0.3121 acc_train: 0.9500 loss_val: 0.6782 acc_val: 0.8267\n",
            "Epoch: 0044 loss_train: 0.2852 acc_train: 0.9714 loss_val: 0.6779 acc_val: 0.8267\n",
            "Epoch: 0045 loss_train: 0.3058 acc_train: 0.9643 loss_val: 0.6776 acc_val: 0.8267\n",
            "Epoch: 0046 loss_train: 0.3217 acc_train: 0.9500 loss_val: 0.6778 acc_val: 0.8267\n",
            "Epoch: 0047 loss_train: 0.3718 acc_train: 0.9429 loss_val: 0.6782 acc_val: 0.8233\n",
            "Epoch: 0048 loss_train: 0.3115 acc_train: 0.9571 loss_val: 0.6788 acc_val: 0.8200\n",
            "Epoch: 0049 loss_train: 0.3071 acc_train: 0.9714 loss_val: 0.6795 acc_val: 0.8200\n",
            "Epoch: 0050 loss_train: 0.3389 acc_train: 0.9500 loss_val: 0.6800 acc_val: 0.8200\n",
            "Epoch: 0051 loss_train: 0.3016 acc_train: 0.9714 loss_val: 0.6807 acc_val: 0.8167\n",
            "Epoch: 0052 loss_train: 0.2944 acc_train: 0.9643 loss_val: 0.6815 acc_val: 0.8200\n",
            "Epoch: 0053 loss_train: 0.3359 acc_train: 0.9500 loss_val: 0.6821 acc_val: 0.8200\n",
            "Epoch: 0054 loss_train: 0.2973 acc_train: 0.9643 loss_val: 0.6823 acc_val: 0.8200\n",
            "Epoch: 0055 loss_train: 0.3393 acc_train: 0.9643 loss_val: 0.6818 acc_val: 0.8200\n",
            "Epoch: 0056 loss_train: 0.2938 acc_train: 0.9643 loss_val: 0.6813 acc_val: 0.8200\n",
            "Epoch: 0057 loss_train: 0.3191 acc_train: 0.9714 loss_val: 0.6810 acc_val: 0.8200\n",
            "Epoch: 0058 loss_train: 0.2893 acc_train: 0.9643 loss_val: 0.6802 acc_val: 0.8233\n",
            "Epoch: 0059 loss_train: 0.3169 acc_train: 0.9643 loss_val: 0.6790 acc_val: 0.8200\n",
            "Epoch: 0060 loss_train: 0.3366 acc_train: 0.9571 loss_val: 0.6776 acc_val: 0.8200\n",
            "Epoch: 0061 loss_train: 0.3353 acc_train: 0.9571 loss_val: 0.6758 acc_val: 0.8200\n",
            "Epoch: 0062 loss_train: 0.3031 acc_train: 0.9500 loss_val: 0.6741 acc_val: 0.8167\n",
            "Epoch: 0063 loss_train: 0.3252 acc_train: 0.9714 loss_val: 0.6726 acc_val: 0.8167\n",
            "Epoch: 0064 loss_train: 0.3527 acc_train: 0.9714 loss_val: 0.6712 acc_val: 0.8167\n",
            "Epoch: 0065 loss_train: 0.3313 acc_train: 0.9500 loss_val: 0.6702 acc_val: 0.8167\n",
            "Epoch: 0066 loss_train: 0.3012 acc_train: 0.9786 loss_val: 0.6690 acc_val: 0.8167\n",
            "Epoch: 0067 loss_train: 0.3210 acc_train: 0.9714 loss_val: 0.6678 acc_val: 0.8167\n",
            "Epoch: 0068 loss_train: 0.2961 acc_train: 0.9643 loss_val: 0.6668 acc_val: 0.8167\n",
            "Epoch: 0069 loss_train: 0.3068 acc_train: 0.9500 loss_val: 0.6660 acc_val: 0.8167\n",
            "Epoch: 0070 loss_train: 0.3164 acc_train: 0.9786 loss_val: 0.6654 acc_val: 0.8167\n",
            "Epoch: 0071 loss_train: 0.2918 acc_train: 0.9643 loss_val: 0.6649 acc_val: 0.8167\n",
            "Epoch: 0072 loss_train: 0.3167 acc_train: 0.9571 loss_val: 0.6644 acc_val: 0.8167\n",
            "Epoch: 0073 loss_train: 0.3148 acc_train: 0.9714 loss_val: 0.6645 acc_val: 0.8133\n",
            "Epoch: 0074 loss_train: 0.2994 acc_train: 0.9786 loss_val: 0.6649 acc_val: 0.8167\n",
            "Epoch: 0075 loss_train: 0.3292 acc_train: 0.9429 loss_val: 0.6659 acc_val: 0.8167\n",
            "Epoch: 0076 loss_train: 0.3381 acc_train: 0.9643 loss_val: 0.6665 acc_val: 0.8167\n",
            "Epoch: 0077 loss_train: 0.2946 acc_train: 0.9643 loss_val: 0.6676 acc_val: 0.8133\n",
            "Epoch: 0078 loss_train: 0.2967 acc_train: 0.9714 loss_val: 0.6683 acc_val: 0.8133\n",
            "Epoch: 0079 loss_train: 0.3328 acc_train: 0.9500 loss_val: 0.6681 acc_val: 0.8133\n",
            "Epoch: 0080 loss_train: 0.3190 acc_train: 0.9714 loss_val: 0.6676 acc_val: 0.8133\n",
            "Epoch: 0081 loss_train: 0.2943 acc_train: 0.9571 loss_val: 0.6677 acc_val: 0.8167\n",
            "Epoch: 0082 loss_train: 0.2876 acc_train: 0.9786 loss_val: 0.6673 acc_val: 0.8167\n",
            "Epoch: 0083 loss_train: 0.2879 acc_train: 0.9929 loss_val: 0.6670 acc_val: 0.8200\n",
            "Epoch: 0084 loss_train: 0.3262 acc_train: 0.9714 loss_val: 0.6669 acc_val: 0.8200\n",
            "Epoch: 0085 loss_train: 0.2742 acc_train: 0.9643 loss_val: 0.6663 acc_val: 0.8200\n",
            "Epoch: 0086 loss_train: 0.3008 acc_train: 0.9714 loss_val: 0.6656 acc_val: 0.8200\n",
            "Epoch: 0087 loss_train: 0.2849 acc_train: 0.9571 loss_val: 0.6650 acc_val: 0.8200\n",
            "Epoch: 0088 loss_train: 0.3228 acc_train: 0.9500 loss_val: 0.6643 acc_val: 0.8233\n",
            "Epoch: 0089 loss_train: 0.2956 acc_train: 0.9571 loss_val: 0.6631 acc_val: 0.8200\n",
            "Epoch: 0090 loss_train: 0.2688 acc_train: 0.9786 loss_val: 0.6624 acc_val: 0.8200\n",
            "Epoch: 0091 loss_train: 0.2903 acc_train: 0.9571 loss_val: 0.6628 acc_val: 0.8200\n",
            "Epoch: 0092 loss_train: 0.3094 acc_train: 0.9643 loss_val: 0.6631 acc_val: 0.8200\n",
            "Epoch: 0093 loss_train: 0.2844 acc_train: 0.9714 loss_val: 0.6636 acc_val: 0.8200\n",
            "Epoch: 0094 loss_train: 0.2763 acc_train: 0.9714 loss_val: 0.6639 acc_val: 0.8167\n",
            "Epoch: 0095 loss_train: 0.3544 acc_train: 0.9357 loss_val: 0.6650 acc_val: 0.8200\n",
            "Epoch: 0096 loss_train: 0.2837 acc_train: 0.9786 loss_val: 0.6656 acc_val: 0.8233\n",
            "Epoch: 0097 loss_train: 0.2977 acc_train: 0.9500 loss_val: 0.6654 acc_val: 0.8233\n",
            "Epoch: 0098 loss_train: 0.2739 acc_train: 0.9857 loss_val: 0.6649 acc_val: 0.8200\n",
            "Epoch: 0099 loss_train: 0.3257 acc_train: 0.9571 loss_val: 0.6640 acc_val: 0.8200\n",
            "Epoch: 0100 loss_train: 0.2752 acc_train: 0.9714 loss_val: 0.6637 acc_val: 0.8200\n",
            "Epoch: 0101 loss_train: 0.2940 acc_train: 0.9571 loss_val: 0.6632 acc_val: 0.8200\n",
            "Epoch: 0102 loss_train: 0.2811 acc_train: 0.9643 loss_val: 0.6628 acc_val: 0.8233\n",
            "Epoch: 0103 loss_train: 0.2897 acc_train: 0.9643 loss_val: 0.6625 acc_val: 0.8233\n",
            "Epoch: 0104 loss_train: 0.2728 acc_train: 0.9714 loss_val: 0.6623 acc_val: 0.8200\n",
            "Epoch: 0105 loss_train: 0.2720 acc_train: 0.9786 loss_val: 0.6620 acc_val: 0.8200\n",
            "Epoch: 0106 loss_train: 0.2844 acc_train: 0.9786 loss_val: 0.6617 acc_val: 0.8200\n",
            "Epoch: 0107 loss_train: 0.2977 acc_train: 0.9500 loss_val: 0.6617 acc_val: 0.8167\n",
            "Epoch: 0108 loss_train: 0.2641 acc_train: 0.9714 loss_val: 0.6610 acc_val: 0.8167\n",
            "Epoch: 0109 loss_train: 0.2746 acc_train: 0.9714 loss_val: 0.6603 acc_val: 0.8167\n",
            "Epoch: 0110 loss_train: 0.2867 acc_train: 0.9714 loss_val: 0.6593 acc_val: 0.8167\n",
            "Epoch: 0111 loss_train: 0.2747 acc_train: 0.9786 loss_val: 0.6579 acc_val: 0.8233\n",
            "Epoch: 0112 loss_train: 0.3165 acc_train: 0.9714 loss_val: 0.6564 acc_val: 0.8233\n",
            "Epoch: 0113 loss_train: 0.2892 acc_train: 0.9857 loss_val: 0.6548 acc_val: 0.8200\n",
            "Epoch: 0114 loss_train: 0.2859 acc_train: 0.9786 loss_val: 0.6529 acc_val: 0.8200\n",
            "Epoch: 0115 loss_train: 0.2787 acc_train: 0.9714 loss_val: 0.6511 acc_val: 0.8200\n",
            "Epoch: 0116 loss_train: 0.2858 acc_train: 0.9714 loss_val: 0.6497 acc_val: 0.8200\n",
            "Epoch: 0117 loss_train: 0.3063 acc_train: 0.9429 loss_val: 0.6490 acc_val: 0.8200\n",
            "Epoch: 0118 loss_train: 0.2859 acc_train: 0.9571 loss_val: 0.6491 acc_val: 0.8200\n",
            "Epoch: 0119 loss_train: 0.2717 acc_train: 0.9714 loss_val: 0.6499 acc_val: 0.8200\n",
            "Epoch: 0120 loss_train: 0.2929 acc_train: 0.9357 loss_val: 0.6512 acc_val: 0.8200\n",
            "Epoch: 0121 loss_train: 0.2781 acc_train: 0.9500 loss_val: 0.6527 acc_val: 0.8233\n",
            "Epoch: 0122 loss_train: 0.2647 acc_train: 0.9857 loss_val: 0.6538 acc_val: 0.8233\n",
            "Epoch: 0123 loss_train: 0.2635 acc_train: 0.9714 loss_val: 0.6552 acc_val: 0.8200\n",
            "Epoch: 0124 loss_train: 0.3081 acc_train: 0.9643 loss_val: 0.6569 acc_val: 0.8200\n",
            "Epoch: 0125 loss_train: 0.2843 acc_train: 0.9857 loss_val: 0.6583 acc_val: 0.8167\n",
            "Epoch: 0126 loss_train: 0.2774 acc_train: 0.9786 loss_val: 0.6590 acc_val: 0.8167\n",
            "Epoch: 0127 loss_train: 0.2636 acc_train: 0.9786 loss_val: 0.6595 acc_val: 0.8167\n",
            "Epoch: 0128 loss_train: 0.2551 acc_train: 0.9714 loss_val: 0.6592 acc_val: 0.8167\n",
            "Epoch: 0129 loss_train: 0.2750 acc_train: 0.9571 loss_val: 0.6578 acc_val: 0.8167\n",
            "Epoch: 0130 loss_train: 0.2459 acc_train: 0.9786 loss_val: 0.6564 acc_val: 0.8167\n",
            "Epoch: 0131 loss_train: 0.2859 acc_train: 0.9643 loss_val: 0.6553 acc_val: 0.8200\n",
            "Epoch: 0132 loss_train: 0.3009 acc_train: 0.9429 loss_val: 0.6545 acc_val: 0.8167\n",
            "Epoch: 0133 loss_train: 0.2690 acc_train: 0.9571 loss_val: 0.6545 acc_val: 0.8167\n",
            "Epoch: 0134 loss_train: 0.2884 acc_train: 0.9429 loss_val: 0.6547 acc_val: 0.8133\n",
            "Epoch: 0135 loss_train: 0.2715 acc_train: 0.9786 loss_val: 0.6548 acc_val: 0.8167\n",
            "Epoch: 0136 loss_train: 0.2405 acc_train: 0.9714 loss_val: 0.6552 acc_val: 0.8167\n",
            "Epoch: 0137 loss_train: 0.2501 acc_train: 0.9714 loss_val: 0.6557 acc_val: 0.8200\n",
            "Epoch: 0138 loss_train: 0.2972 acc_train: 0.9786 loss_val: 0.6565 acc_val: 0.8167\n",
            "Epoch: 0139 loss_train: 0.3036 acc_train: 0.9571 loss_val: 0.6568 acc_val: 0.8167\n",
            "Epoch: 0140 loss_train: 0.2995 acc_train: 0.9643 loss_val: 0.6567 acc_val: 0.8200\n",
            "Epoch: 0141 loss_train: 0.2825 acc_train: 0.9786 loss_val: 0.6570 acc_val: 0.8200\n",
            "Epoch: 0142 loss_train: 0.2821 acc_train: 0.9714 loss_val: 0.6577 acc_val: 0.8167\n",
            "Epoch: 0143 loss_train: 0.2770 acc_train: 0.9571 loss_val: 0.6581 acc_val: 0.8167\n",
            "Epoch: 0144 loss_train: 0.2914 acc_train: 0.9571 loss_val: 0.6585 acc_val: 0.8200\n",
            "Epoch: 0145 loss_train: 0.2576 acc_train: 0.9786 loss_val: 0.6591 acc_val: 0.8200\n",
            "Epoch: 0146 loss_train: 0.3122 acc_train: 0.9571 loss_val: 0.6590 acc_val: 0.8167\n",
            "Epoch: 0147 loss_train: 0.2314 acc_train: 0.9643 loss_val: 0.6589 acc_val: 0.8133\n",
            "Epoch: 0148 loss_train: 0.2549 acc_train: 0.9714 loss_val: 0.6587 acc_val: 0.8133\n",
            "Epoch: 0149 loss_train: 0.2670 acc_train: 0.9786 loss_val: 0.6584 acc_val: 0.8133\n",
            "Epoch: 0150 loss_train: 0.2527 acc_train: 0.9571 loss_val: 0.6589 acc_val: 0.8167\n",
            "Epoch: 0151 loss_train: 0.2704 acc_train: 0.9571 loss_val: 0.6599 acc_val: 0.8167\n",
            "Epoch: 0152 loss_train: 0.2776 acc_train: 0.9786 loss_val: 0.6601 acc_val: 0.8167\n",
            "Epoch: 0153 loss_train: 0.2847 acc_train: 0.9571 loss_val: 0.6595 acc_val: 0.8200\n",
            "Epoch: 0154 loss_train: 0.2849 acc_train: 0.9714 loss_val: 0.6579 acc_val: 0.8200\n",
            "Epoch: 0155 loss_train: 0.2666 acc_train: 0.9643 loss_val: 0.6554 acc_val: 0.8133\n",
            "Epoch: 0156 loss_train: 0.2978 acc_train: 0.9571 loss_val: 0.6537 acc_val: 0.8133\n",
            "Epoch: 0157 loss_train: 0.2596 acc_train: 0.9714 loss_val: 0.6523 acc_val: 0.8133\n",
            "Epoch: 0158 loss_train: 0.2777 acc_train: 0.9786 loss_val: 0.6499 acc_val: 0.8133\n",
            "Epoch: 0159 loss_train: 0.2598 acc_train: 0.9857 loss_val: 0.6483 acc_val: 0.8167\n",
            "Epoch: 0160 loss_train: 0.2955 acc_train: 0.9571 loss_val: 0.6476 acc_val: 0.8200\n",
            "Epoch: 0161 loss_train: 0.2462 acc_train: 0.9643 loss_val: 0.6479 acc_val: 0.8200\n",
            "Epoch: 0162 loss_train: 0.2958 acc_train: 0.9500 loss_val: 0.6483 acc_val: 0.8200\n",
            "Epoch: 0163 loss_train: 0.2687 acc_train: 0.9643 loss_val: 0.6491 acc_val: 0.8200\n",
            "Epoch: 0164 loss_train: 0.2770 acc_train: 0.9714 loss_val: 0.6492 acc_val: 0.8167\n",
            "Epoch: 0165 loss_train: 0.2548 acc_train: 0.9857 loss_val: 0.6487 acc_val: 0.8200\n",
            "Epoch: 0166 loss_train: 0.3002 acc_train: 0.9500 loss_val: 0.6487 acc_val: 0.8200\n",
            "Epoch: 0167 loss_train: 0.2987 acc_train: 0.9714 loss_val: 0.6492 acc_val: 0.8200\n",
            "Epoch: 0168 loss_train: 0.2890 acc_train: 0.9643 loss_val: 0.6502 acc_val: 0.8200\n",
            "Epoch: 0169 loss_train: 0.2895 acc_train: 0.9714 loss_val: 0.6506 acc_val: 0.8167\n",
            "Epoch: 0170 loss_train: 0.2506 acc_train: 0.9643 loss_val: 0.6510 acc_val: 0.8167\n",
            "Epoch: 0171 loss_train: 0.2587 acc_train: 0.9786 loss_val: 0.6500 acc_val: 0.8167\n",
            "Epoch: 0172 loss_train: 0.2710 acc_train: 0.9786 loss_val: 0.6486 acc_val: 0.8200\n",
            "Epoch: 0173 loss_train: 0.2300 acc_train: 0.9714 loss_val: 0.6471 acc_val: 0.8200\n",
            "Epoch: 0174 loss_train: 0.2703 acc_train: 0.9714 loss_val: 0.6454 acc_val: 0.8200\n",
            "Epoch: 0175 loss_train: 0.2644 acc_train: 0.9786 loss_val: 0.6438 acc_val: 0.8200\n",
            "Epoch: 0176 loss_train: 0.2503 acc_train: 0.9786 loss_val: 0.6429 acc_val: 0.8200\n",
            "Epoch: 0177 loss_train: 0.2748 acc_train: 0.9714 loss_val: 0.6415 acc_val: 0.8233\n",
            "Epoch: 0178 loss_train: 0.2803 acc_train: 0.9643 loss_val: 0.6403 acc_val: 0.8200\n",
            "Epoch: 0179 loss_train: 0.2828 acc_train: 0.9714 loss_val: 0.6394 acc_val: 0.8200\n",
            "Epoch: 0180 loss_train: 0.2700 acc_train: 0.9571 loss_val: 0.6384 acc_val: 0.8200\n",
            "Epoch: 0181 loss_train: 0.2365 acc_train: 0.9786 loss_val: 0.6379 acc_val: 0.8200\n",
            "Epoch: 0182 loss_train: 0.2335 acc_train: 0.9857 loss_val: 0.6380 acc_val: 0.8233\n",
            "Epoch: 0183 loss_train: 0.2858 acc_train: 0.9571 loss_val: 0.6383 acc_val: 0.8267\n",
            "Epoch: 0184 loss_train: 0.2974 acc_train: 0.9786 loss_val: 0.6390 acc_val: 0.8233\n",
            "Epoch: 0185 loss_train: 0.2736 acc_train: 0.9786 loss_val: 0.6403 acc_val: 0.8233\n",
            "Epoch: 0186 loss_train: 0.2500 acc_train: 0.9643 loss_val: 0.6416 acc_val: 0.8233\n",
            "Epoch: 0187 loss_train: 0.2580 acc_train: 0.9571 loss_val: 0.6431 acc_val: 0.8167\n",
            "Epoch: 0188 loss_train: 0.2502 acc_train: 0.9571 loss_val: 0.6447 acc_val: 0.8133\n",
            "Epoch: 0189 loss_train: 0.2566 acc_train: 0.9571 loss_val: 0.6457 acc_val: 0.8133\n",
            "Epoch: 0190 loss_train: 0.2207 acc_train: 0.9714 loss_val: 0.6468 acc_val: 0.8133\n",
            "Epoch: 0191 loss_train: 0.2874 acc_train: 0.9786 loss_val: 0.6466 acc_val: 0.8133\n",
            "Epoch: 0192 loss_train: 0.2591 acc_train: 0.9714 loss_val: 0.6460 acc_val: 0.8133\n",
            "Epoch: 0193 loss_train: 0.2351 acc_train: 0.9857 loss_val: 0.6450 acc_val: 0.8167\n",
            "Epoch: 0194 loss_train: 0.3016 acc_train: 0.9571 loss_val: 0.6444 acc_val: 0.8167\n",
            "Epoch: 0195 loss_train: 0.2561 acc_train: 0.9786 loss_val: 0.6432 acc_val: 0.8167\n",
            "Epoch: 0196 loss_train: 0.2826 acc_train: 0.9571 loss_val: 0.6426 acc_val: 0.8167\n",
            "Epoch: 0197 loss_train: 0.2470 acc_train: 0.9643 loss_val: 0.6424 acc_val: 0.8133\n",
            "Epoch: 0198 loss_train: 0.2735 acc_train: 0.9643 loss_val: 0.6422 acc_val: 0.8167\n",
            "Epoch: 0199 loss_train: 0.2243 acc_train: 0.9857 loss_val: 0.6422 acc_val: 0.8167\n",
            "Epoch: 0200 loss_train: 0.2596 acc_train: 0.9857 loss_val: 0.6425 acc_val: 0.8200\n",
            "Optimization Finished!\n",
            "Total time elapsed: 1.2892s\n",
            "Wall time: 1.29 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Train model\n",
        "t_total = time.time()\n",
        "for epoch in range(EPOCH):\n",
        "    train(epoch)\n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTftkRzPbrmG"
      },
      "source": [
        "### **Test**\n",
        "When I measure test time, About 6.79 sec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "id": "vS2zkuFmal0I",
        "outputId": "369f0745-8860-4218-e4c8-7175dfac52b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding shape: [1000, 7]\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-86-b5a75d997919>\u001b[0m in \u001b[0;36mtest\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mvisualize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mloss_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0macc_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-86-b5a75d997919>\u001b[0m in \u001b[0;36mvisualize\u001b[1;34m(h, label, idx)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Embedding shape: {list(h_.shape)}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m70\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Set2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mscatter\u001b[1;34m(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, plotnonfinite, data, **kwargs)\u001b[0m\n\u001b[0;32m   2888\u001b[0m         \u001b[0mverts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeprecation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_deprecated_parameter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2889\u001b[0m         edgecolors=None, *, plotnonfinite=False, data=None, **kwargs):\n\u001b[1;32m-> 2890\u001b[1;33m     __ret = gca().scatter(\n\u001b[0m\u001b[0;32m   2891\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmarker\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcmap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2892\u001b[0m         \u001b[0mvmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvmax\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinewidths\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlinewidths\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1446\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1447\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[0mbound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\cbook\\deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[0;32m    409\u001b[0m                          \u001b[1;32melse\u001b[0m \u001b[0mdeprecation_addendum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m                 **kwargs)\n\u001b[1;32m--> 411\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0minner_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    412\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mscatter\u001b[1;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, plotnonfinite, **kwargs)\u001b[0m\n\u001b[0;32m   4449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4450\u001b[0m         \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medgecolors\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4451\u001b[1;33m             self._parse_scatter_color_args(\n\u001b[0m\u001b[0;32m   4452\u001b[0m                 \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medgecolors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4453\u001b[0m                 get_next_color_func=self._get_patches_for_fill.get_next_color)\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36m_parse_scatter_color_args\u001b[1;34m(c, edgecolors, kwargs, xsize, get_next_color_func)\u001b[0m\n\u001b[0;32m   4261\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mc_was_none\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mkwcolor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mc_is_string_or_strings\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4262\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# First, does 'c' look suitable for value-mapping?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4263\u001b[1;33m                 \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4264\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4265\u001b[0m                 \u001b[1;32mpass\u001b[0m  \u001b[1;31m# Failed to convert to float array; must be color specs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masanyarray\u001b[1;34m(a, dtype, order, like)\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_asanyarray_with_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    643\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 645\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m     \u001b[1;31m# Wrap Numpy array again in a suitable tensor when done, to support e.g.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Testing\n",
        "test()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LD0fdcPL5XZ1"
      },
      "source": [
        "## **3. (HOMEWORK) Graph Classification on Collab Dataset**\n",
        "\n",
        "**Collab Dataset** <br>\n",
        "it is a large dataset containing many graphs and graph labels. <br>\n",
        "This dataset is mainly used for graph classification.<br> <br>\n",
        "\n",
        "The code is made on pytorch_geometric library. <br>\n",
        "**Why do I use?** <br>\n",
        "pytorch_geometric is very fast despite working on sparse data. Compared to the Deep GraphLibrary (DGL) 0.1.3, pytorch_geometric trains models up to 15 times faster. <br>\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=13jMho-M4Em8B32HzNWkXpnUcA4QGyXYU)\n",
        "\n",
        "So, I recommend running the code and studying the library. <br><br>\n",
        "\n",
        "Reference : https://medium.com/syncedreview/pytorch-geometric-a-fast-pytorch-library-for-dl-a833dff466e5\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iV8GudwXVSQn"
      },
      "source": [
        "### **Prem**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKv-FxdnVOdi"
      },
      "outputs": [],
      "source": [
        "!pip install torch-scatter==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.7.0.html\n",
        "!pip install torch-sparse==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.7.0.html\n",
        "!pip install torch-cluster==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.7.0.html\n",
        "!pip install torch-spline-conv==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.7.0.html\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "my0_ON_jsrkJ"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.utils import to_networkx\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.utils import degree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyWTw6hc-LQ1",
        "outputId": "d13192e3-1676-4c62-fc89-078b08efa8b2"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "def create_one_hot_transform(dataset): # Since the collab dataset does not have a node feature, So I make a node feature using the max_degree value.\n",
        "    max_degree = 0                     # I reference that in https://paperswithcode.com/sota/graph-classification-on-collab.\n",
        "    degs = []\n",
        "    for data in dataset:\n",
        "        degs += [degree(data.edge_index[0], dtype=torch.long)]\n",
        "        max_degree = max(max_degree, degs[-1].max().item())\n",
        "\n",
        "    return T.OneHotDegree(max_degree)\n",
        "\n",
        "def load_dataset():\n",
        "        dataset = TUDataset(root='/tmp/COLLAB', name=\"COLLAB\")\n",
        "        dataset.transform = create_one_hot_transform(dataset)\n",
        "        return dataset\n",
        "\n",
        "dataset = load_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w50MQPByDH7S",
        "outputId": "aa1fe3d1-a313-4e36-e2f5-22a5bea4e396"
      },
      "outputs": [],
      "source": [
        "print()\n",
        "print(f'Dataset: {dataset}:')\n",
        "print('====================')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of classes: {dataset.num_classes}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "\n",
        "\n",
        "\n",
        "###### One graph #####\n",
        "data = dataset[0]  # Get the first graph object.\n",
        "\n",
        "print()\n",
        "print(data)\n",
        "print('=============================================================')\n",
        "\n",
        "# Gather some statistics about the first graph.\n",
        "print(f'Number of nodes: {data.num_nodes}')\n",
        "print(f'Number of edges: {data.num_edges}')\n",
        "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
        "print(f'Contains isolated nodes: {data.contains_isolated_nodes()}')\n",
        "print(f'Contains self-loops: {data.contains_self_loops()}')\n",
        "print(f'Is undirected: {data.is_undirected()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jf3KmCfkgQr",
        "outputId": "ad0b2639-ed75-4bdd-97ac-4a2fc98a4e6d"
      },
      "outputs": [],
      "source": [
        "# One graph edges\n",
        "print(data.edge_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        },
        "id": "dgDmiSYohab7",
        "outputId": "d9fc32c3-6595-468a-831e-adb01376cb9b"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.utils import to_networkx\n",
        "\n",
        "G = to_networkx(data, to_undirected=True)\n",
        "\n",
        "#It shows one graph of Collab dataset.\n",
        "plt.figure(figsize=(10, 10))\n",
        "nx.draw_networkx(G, pos=nx.spring_layout(G, seed=42), with_labels=True, cmap=\"Set2\", width=0.5, node_size=500, node_color='yellow')\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDBlvYYTG1Lp",
        "outputId": "017c0f71-7b1f-4be9-e642-4c3f93fd50b4"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(12345)\n",
        "dataset = dataset.shuffle() # Label data are sequentially located. (0, 1, 2)\n",
        "\n",
        "# train / valid\n",
        "train_dataset = dataset[:4000]\n",
        "valid_dataset = dataset[4000:]\n",
        "\n",
        "print(f'Number of training graphs: {len(train_dataset)}')\n",
        "print(f'Number of test graphs: {len(valid_dataset)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_OvsocSHUF8"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.data import DataLoader\n",
        "\n",
        "# Unlike CV and NLP, in graph, DataLoader aggregates node_feature, weight and edge_index from different samples/ graphs into Batches\n",
        "# So The GNN model needs this “batch” information to know which nodes belong to the same graph within a batch to perform computation. \n",
        "# Reference : https://towardsdatascience.com/hands-on-graph-neural-networks-with-pytorch-pytorch-geometric-359487e221a8\n",
        "# Reference : https://pytorch-geometric.readthedocs.io/en/latest/notes/batching.html\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UlfB3TOUDDJ"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GCNConv(hidden_channels, hidden_channels) # When I used one more GCNConv, the performance came out better.\n",
        "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        # 1. Obtain node embeddings \n",
        "        #ipdb.set_trace()\n",
        "        \n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        # 2. Readout layer\n",
        "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels] , for graph classsification\n",
        "        h = x.clone().detach() # for making graph embedding        \n",
        "\n",
        "        # 3. Apply a final classifier\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.lin(x)\n",
        "        \n",
        "        return x , h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCsUYOZi0Tw8"
      },
      "outputs": [],
      "source": [
        "model = GCN(hidden_channels=64)\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train(epoch=None):\n",
        "    model.train()\n",
        "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
        "         data = data.to(device)\n",
        "         out, _ = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
        "         loss = criterion(out, data.y)  # Compute the loss.\n",
        "         loss.backward()  # Derive gradients.\n",
        "         optimizer.step()  # Update parameters based on gradients.\n",
        "         optimizer.zero_grad()  # Clear gradients.\n",
        "    \n",
        "    print(f'Epoch: {epoch:03d}, Train loss: {loss:.4f}')\n",
        "         \n",
        "\n",
        "def test(loader, visual=False):\n",
        "  model.eval()\n",
        "  \n",
        "  correct = 0\n",
        "  for data in loader:  # Iterate in batches over the training/test dataset.\n",
        "    data = data.to(device)\n",
        "    out, h = model(data.x, data.edge_index, data.batch)\n",
        "    pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "    correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
        "    \n",
        "    if visual == True:\n",
        "      colors = ['#3A3120', '#535D8E', '#BD3430']\n",
        "      color = [ colors[i] for i in data.y.detach().cpu()]\n",
        "      z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy())\n",
        "      \n",
        "      plt.figure(figsize=(10,10))\n",
        "      plt.xticks([])\n",
        "      plt.yticks([])\n",
        "      print(f'Embedding shape: {list(h.shape)}')\n",
        "      plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
        "      plt.show()\n",
        "\n",
        "  return correct / len(loader.dataset)  # Derive ratio of correct predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqvtRCxXFE9U",
        "outputId": "15dd85fb-d9b4-4362-f86a-3b3d413e7cb6"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "################################\n",
        "for epoch in range(1, 31):\n",
        "    train(epoch)\n",
        "    test_acc = test(valid_loader)\n",
        "    if epoch % 5 == 0:\n",
        "      print(f'Epoch: {epoch:03d}, Test Acc: {test_acc:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n68qWvinfz4e"
      },
      "outputs": [],
      "source": [
        "### When you run the code, uncomment below command.\n",
        "#test(valid_loader, visual=True) # t-SNE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aMw2S5KdVZ_"
      },
      "source": [
        "## **Reference**\n",
        "Thomas N. Kipf, Max Welling, Semi-Supervised Classification with Graph Convolutional Networks (ICLR 2017) <br>\n",
        "http://tkipf.github.io/graph-convolutional-networks/ <br>\n",
        "https://relational.fit.cvut.cz/dataset/CORA <br>\n",
        "https://paperswithcode.com/sota/graph-classification-on-collab <br>\n",
        "https://pytorch-geometric.readthedocs.io/en/latest/notes/batching.html <br>\n",
        "https://medium.com/syncedreview/pytorch-geometric-a-fast-pytorch-library-for-dl-a833dff466e5 <br>\n",
        "https://towardsdatascience.com/hands-on-graph-neural-networks-with-pytorch-pytorch-geometric-359487e221a8 <br>\n",
        "https://paperswithcode.com/sota/node-classification-on-cora <br>\n",
        "https://graphsandnetworks.com/the-cora-dataset/ <br>\n",
        "https://github.com/tkipf/pygcn <br>\n",
        "https://pytorch-geometric.readthedocs.io/en/latest/ <br>\n",
        "https://colab.research.google.com/drive/1I8a0DfQ3fI7Njc62__mVXUlcAleUclnb?usp=sharing <br>\n",
        "http://networkrepository.com/COLLAB.php <br>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "ai504_13_GNN_practice.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okJXS1-OEbN7"
      },
      "source": [
        "# Table of contents\n",
        "1. [Prepare input](#1)\n",
        "2. [Implement Transformer](#2)\n",
        "3. [Train and Evaluate](#3)\n",
        "4. [Visualize attention](#4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2Bj-rF9EbN7"
      },
      "source": [
        "# Prepare essential packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHrI30vIEbN7",
        "outputId": "840115c7-414b-4725-9f2e-36a958d84312",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchtext in c:\\programdata\\anaconda3\\lib\\site-packages (0.4.0)\n",
            "Collecting torchtext\n",
            "  Using cached torchtext-0.11.2-cp38-cp38-win_amd64.whl (1.5 MB)\n",
            "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from torchtext) (2.25.1)\n",
            "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from torchtext) (4.59.0)\n",
            "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from torchtext) (1.20.1)\n",
            "Collecting torch==1.10.2\n",
            "  Using cached torch-1.10.2-cp38-cp38-win_amd64.whl (226.6 MB)\n",
            "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==1.10.2->torchtext) (3.7.4.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchtext) (1.26.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchtext) (2020.12.5)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchtext) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchtext) (2.10)\n",
            "Installing collected packages: torch, torchtext\n",
            "Successfully installed torch-1.10.2 torchtext-0.11.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -n-core-web-sm (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -cipy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -cikit-image (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -n-core-web-sm (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -cipy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -cikit-image (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -n-core-web-sm (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -cipy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -cikit-image (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "  WARNING: The scripts convert-caffe2-to-onnx.exe, convert-onnx-to-caffe2.exe and torchrun.exe are installed in 'C:\\Users\\hiu06\\AppData\\Roaming\\Python\\Python38\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.10.1 requires torch==1.9.1, but you have torch 1.10.2 which is incompatible.\n",
            "torchaudio 0.9.1 requires torch==1.9.1, but you have torch 1.10.2 which is incompatible.\n",
            "WARNING: Ignoring invalid distribution -n-core-web-sm (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -cipy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -cikit-image (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -n-core-web-sm (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -cipy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -cikit-image (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -n-core-web-sm (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -cipy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -cikit-image (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "fatal: destination path 'attentionviz' already exists and is not an empty directory.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⚠ As of spaCy v3.0, shortcuts like 'de' are deprecated. Please use the full"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -n-core-web-sm (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -cipy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -cikit-image (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -n-core-web-sm (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -cipy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -cikit-image (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -n-core-web-sm (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -cipy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -cikit-image (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -n-core-web-sm (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -cipy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -cikit-image (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -n-core-web-sm (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -cipy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -cikit-image (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -n-core-web-sm (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -cipy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -cikit-image (c:\\programdata\\anaconda3\\lib\\site-packages)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "pipeline package name 'de_core_news_sm' instead.\n",
            "Collecting de-core-news-sm==3.2.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.2.0/de_core_news_sm-3.2.0-py3-none-any.whl (19.1 MB)\n",
            "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from de-core-news-sm==3.2.0) (3.2.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.4.0)\n",
            "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.11.3)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (8.0.13)\n",
            "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (52.0.0.post20210125)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (4.59.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.4.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: pathy>=0.3.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.6.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.0.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (20.9)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.7.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.20.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.8)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.9.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.25.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.0.6)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.4.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.7.4.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.26.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.1.1)\n",
            "✔ Download and installation successful\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the full"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -n-core-web-sm (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -cipy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -cikit-image (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -n-core-web-sm (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -cipy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -cikit-image (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -n-core-web-sm (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -cipy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -cikit-image (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "    WARNING: Ignoring invalid distribution -n-core-web-sm (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "    WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "    WARNING: Ignoring invalid distribution -cipy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "    WARNING: Ignoring invalid distribution -cikit-image (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "ERROR: Could not install packages due to an OSError: [WinError 5] 액세스가 거부되었습니다: 'c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages\\\\en_core_web_sm-2.3.1.dist-info\\\\direct_url.json'\n",
            "Consider using the `--user` option or check the permissions.\n",
            "\n",
            "WARNING: Ignoring invalid distribution -n-core-web-sm (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -cipy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -cikit-image (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -n-core-web-sm (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -cipy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -cikit-image (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -n-core-web-sm (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -cipy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -cikit-image (c:\\programdata\\anaconda3\\lib\\site-packages)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "pipeline package name 'en_core_web_sm' instead.\n",
            "Collecting en-core-web-sm==3.2.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
            "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.2.0) (3.2.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.59.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.25.1)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (20.9)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.20.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: pathy>=0.3.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.0)\n",
            "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (52.0.0.post20210125)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.13)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.7.4.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.1.1)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en-core-web-sm 2.3.1\n",
            "    Uninstalling en-core-web-sm-2.3.1:\n"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "!pip install --user --upgrade torchtext\n",
        "!git clone https://github.com/sjpark9503/attentionviz.git\n",
        "!python -m spacy download de\n",
        "!python -m spacy download en"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sayXpp8FEbN8"
      },
      "source": [
        "# I. Prepare input\n",
        "<a id='1'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KWOpLYfEbN8"
      },
      "source": [
        "We've already learned how to preprocess the text data in week 8, 9 & 10.\n",
        "\n",
        "You can see some detailed explanation about translation datasets in [torchtext](https://pytorch.org/text/), [practice session,week 9](https://classum.com/main/course/7726/103) and [PyTorch NMT tutorial](https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaduS25kEbN8",
        "outputId": "6d500f24-79d1-4855-840d-fb770456fd24",
        "scrolled": true
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "[E941] Can't find model 'de'. It looks like you're trying to load a model from a shortcut, which is obsolete as of spaCy v3.0. To load the model, use its full name instead:\n\nnlp = spacy.load(\"de_core_news_sm\")\n\nFor more details on the available models, see the models directory: https://spacy.io/models. If you want to create a blank model, use spacy.blank: nlp = spacy.blank(\"de\")",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-2-1c61f168c4d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mField\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBucketIterator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m SRC = Field(tokenize = \"spacy\",\n\u001b[0m\u001b[0;32m      6\u001b[0m             \u001b[0mtokenizer_language\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"de\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0minit_token\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'<sos>'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchtext\\data\\field.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sequential, use_vocab, init_token, eos_token, fix_length, dtype, preprocessing, postprocessing, lower, tokenize, tokenizer_language, include_lengths, batch_first, pad_token, unk_token, pad_first, truncate_first, stop_words, is_target)\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;31m# in case the tokenizer isn't picklable (e.g. spacy)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer_language\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer_language\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minclude_lengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minclude_lengths\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_first\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchtext\\data\\utils.py\u001b[0m in \u001b[0;36mget_tokenizer\u001b[1;34m(tokenizer, language)\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m             \u001b[0mspacy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_spacy_tokenize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mspacy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \"\"\"\n\u001b[1;32m---> 51\u001b[1;33m     return util.load_model(\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     )\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m    424\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[index]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    427\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mOSError\u001b[0m: [E941] Can't find model 'de'. It looks like you're trying to load a model from a shortcut, which is obsolete as of spaCy v3.0. To load the model, use its full name instead:\n\nnlp = spacy.load(\"de_core_news_sm\")\n\nFor more details on the available models, see the models directory: https://spacy.io/models. If you want to create a blank model, use spacy.blank: nlp = spacy.blank(\"de\")"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "\n",
        "SRC = Field(tokenize = \"spacy\",\n",
        "            tokenizer_language=\"de\",\n",
        "            init_token = '<sos>',\n",
        "            eos_token = '<eos>',\n",
        "            batch_first=True,\n",
        "            lower = True)\n",
        "\n",
        "TRG = Field(tokenize = \"spacy\",\n",
        "            tokenizer_language=\"en\",\n",
        "            init_token = '<sos>',\n",
        "            eos_token = '<eos>',\n",
        "            batch_first=True,\n",
        "            lower = True)\n",
        "\n",
        "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),\n",
        "                                                    fields = (SRC, TRG))\n",
        "\n",
        "SRC.build_vocab(train_data, min_freq = 2)\n",
        "TRG.build_vocab(train_data, min_freq = 2)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device,\n",
        "    shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE_u1Qg-EbN8"
      },
      "source": [
        "# II. Implement Transformer\n",
        "<a id='2'></a>\n",
        "In practice week 11, we will learn how to implement the __[Attention is all you need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) (Vaswani et al., 2017)__\n",
        "\n",
        "The overall architecutre is as follows:\n",
        "![picture](http://incredible.ai/assets/images/transformer-architecture.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqEVVfl-EbN8"
      },
      "source": [
        "## 1. Basic building blocks\n",
        "\n",
        "In this sections, we will implement the building blocks of the transformer: [Multi-head attention](#1a), [Position wise feedforward network](#1b) and [Positional encoding](#1c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeI2oINrEbN8"
      },
      "source": [
        "### a. Attention\n",
        "<a id='1a'></a>\n",
        "In this section, you will implement scaled dot-product attention and multi-head attention.\n",
        "\n",
        "__Scaled dot product:__\n",
        "![picture](http://incredible.ai/assets/images/transformer-scaled-dot-product.png)\n",
        "__Multi-head attention:__\n",
        "![picture](http://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png)\n",
        "Equation:\n",
        "\n",
        "$$\\begin{align} \\text{MultiHead}(Q, K, V) &= \\text{Concat}(head_1, ...., head_h) W^O \\\\\n",
        "\\text{where head}_i &= \\text{Attention} \\left( QW^Q_i, K W^K_i, VW^v_i \\right)\n",
        "\\end{align}$$\n",
        "__Query, Key and Value projection:__\n",
        "![picture](http://jalammar.github.io/images/t/self-attention-matrix-calculation.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07AkqQcqEbN8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        emb_dim,\n",
        "        num_heads,\n",
        "        dropout=0.0,\n",
        "        bias=False,\n",
        "        encoder_decoder_attention=False,  # otherwise self_attention\n",
        "        causal = False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.emb_dim = emb_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = dropout\n",
        "        self.head_dim = emb_dim // num_heads\n",
        "        assert self.head_dim * num_heads == self.emb_dim, \"emb_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.encoder_decoder_attention = encoder_decoder_attention\n",
        "        self.causal = causal\n",
        "        self.k_proj = nn.Linear(emb_dim, emb_dim, bias=bias)\n",
        "        self.v_proj = nn.Linear(emb_dim, emb_dim, bias=bias)\n",
        "        self.q_proj = nn.Linear(emb_dim, emb_dim, bias=bias)\n",
        "        self.out_proj = nn.Linear(emb_dim, emb_dim, bias=bias)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (\n",
        "            self.num_heads,\n",
        "            self.head_dim,\n",
        "        )\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "        # This is equivalent to\n",
        "        # return x.transpose(1,2)\n",
        "    \n",
        "    def scaled_dot_product(self, \n",
        "                           query: torch.Tensor, \n",
        "                           key: torch.Tensor, \n",
        "                           value: torch.Tensor,\n",
        "                           attention_mask: torch.BoolTensor):\n",
        "\n",
        "        attn_weights = torch.matmul(query, key.transpose(-1, -2)) / math.sqrt(self.emb_dim)\n",
        "        \n",
        "        if attention_mask is not None:\n",
        "            attn_weights = attn_weights.masked_fill(attention_mask.unsqueeze(1), float(\"-inf\"))\n",
        "\n",
        "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
        "        attn_probs = F.dropout(attn_weights, p=self.dropout, training=self.training)\n",
        "\n",
        "        return attn_output, attn_weights\n",
        "    \n",
        "    def MultiHead_scaled_dot_product(self, \n",
        "                       query: torch.Tensor, \n",
        "                       key: torch.Tensor, \n",
        "                       value: torch.Tensor,\n",
        "                       attention_mask: torch.BoolTensor):\n",
        "\n",
        "        attn_weights = torch.matmul(query, key.transpose(-1, -2)) / math.sqrt(self.head_dim)\n",
        "        \n",
        "        if attention_mask is not None:\n",
        "            if self.causal:\n",
        "                attn_weights = attn_weights.masked_fill(attention_mask.unsqueeze(0).unsqueeze(1), float(\"-inf\"))\n",
        "            else:\n",
        "                attn_weights = attn_weights.masked_fill(attention_mask.unsqueeze(1).unsqueeze(2), float(\"-inf\"))\n",
        "\n",
        "\n",
        "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
        "        attn_probs = F.dropout(attn_weights, p=self.dropout, training=self.training)\n",
        "\n",
        "        attn_output = torch.matmul(attn_probs, value)\n",
        "        attn_output = attn_output.permute(0, 2, 1, 3).contiguous()\n",
        "        concat_attn_output_shape = attn_output.size()[:-2] + (self.emb_dim,)\n",
        "        attn_output = attn_output.view(*concat_attn_output_shape)\n",
        "        attn_output = self.out_proj(attn_output)\n",
        "\n",
        "        return attn_output, attn_weights\n",
        "\n",
        "        \n",
        "    def forward(\n",
        "        self,\n",
        "        query: torch.Tensor,\n",
        "        key: torch.Tensor,\n",
        "        attention_mask: torch.Tensor = None,\n",
        "        ):\n",
        "\n",
        "        q = self.q_proj(query)\n",
        "        # Enc-Dec attention\n",
        "        if self.encoder_decoder_attention:\n",
        "            k = self.k_proj(key)\n",
        "            v = self.v_proj(key)\n",
        "        # Self attention\n",
        "        else:\n",
        "            k = self.k_proj(query)\n",
        "            v = self.v_proj(query)\n",
        "\n",
        "        q = self.transpose_for_scores(q)\n",
        "        k = self.transpose_for_scores(k)\n",
        "        v = self.transpose_for_scores(v)\n",
        "\n",
        "        attn_output, attn_weights = self.MultiHead_scaled_dot_product(q,k,v,attention_mask)\n",
        "        return attn_output, attn_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b528gtwHEbN8"
      },
      "source": [
        "### b. Position-wise feed forward network\n",
        "<a id='1b'></a>\n",
        "In this section, we will implement position-wise feed forward network\n",
        "\n",
        "$$\\text{FFN}(x) = \\max \\left(0, x W_1 + b_1 \\right) W_2 + b_2$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBqWWdIyEbN8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, emb_dim: int, d_ff: int, dropout: float = 0.1):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "\n",
        "        self.activation = nn.ReLU()\n",
        "        self.w_1 = nn.Linear(emb_dim, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, emb_dim)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = self.activation(self.w_1(x))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.w_2(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return x + residual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9-qkoUKEbN8"
      },
      "source": [
        "### c. Sinusoidal Positional Encoding\n",
        "<a id='1c'></a>\n",
        "In this section, we will implement sinusoidal positional encoding\n",
        "\n",
        "$$\\begin{align}\n",
        "PE(pos, 2i) &= \\sin \\left( pos / 10000^{2i / d_{model}} \\right)  \\\\\n",
        "PE(pos, 2i+1) &= \\cos \\left( pos / 10000^{2i / d_{model}} \\right)  \n",
        "\\end{align}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsiJalEvEbN8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class SinusoidalPositionalEmbedding(nn.Embedding):\n",
        "\n",
        "    def __init__(self, num_positions, embedding_dim, padding_idx=None):\n",
        "        super().__init__(num_positions, embedding_dim)\n",
        "        self.weight = self._init_weight(self.weight)\n",
        "    \n",
        "    @staticmethod\n",
        "    def _init_weight(out: nn.Parameter):\n",
        "        n_pos, embed_dim = out.shape\n",
        "        pe = nn.Parameter(torch.zeros(out.shape))\n",
        "        for pos in range(n_pos):\n",
        "            for i in range(0, embed_dim, 2):\n",
        "                pe[pos, i] = np.sin(pos / (10000 ** ((2 * i) / embed_dim)))\n",
        "                pe[pos, i + 1] = np.cos(pos / (10000 ** ((2 * (i + 1)) / embed_dim)))\n",
        "        pe.detach_()\n",
        "                \n",
        "        return pe\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, input_ids):\n",
        "        bsz, seq_len = input_ids.shape[:2]\n",
        "        positions = torch.arange(seq_len, dtype=torch.long, device=self.weight.device)\n",
        "        return super().forward(positions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdhwI3hPEbN8"
      },
      "source": [
        "## 2. Transformer Encoder\n",
        "\n",
        "Now we have all basic building blocks which are essential to build Transformer. \n",
        "\n",
        "Let's implement Transformer step-by-step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6ym2hKzEbN8"
      },
      "source": [
        "### a. Encoder layer\n",
        "In this section, we will implement single layer of Transformer encoder.\n",
        "![picture](https://www.researchgate.net/publication/334288604/figure/fig1/AS:778232232148992@1562556431066/The-Transformer-encoder-structure.ppm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6B93kjUlEbN8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.emb_dim = config.emb_dim\n",
        "        self.ffn_dim = config.ffn_dim\n",
        "        self.self_attn = MultiHeadAttention(            \n",
        "            emb_dim=self.emb_dim,\n",
        "            num_heads=config.attention_heads, \n",
        "            dropout=config.attention_dropout)\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(self.emb_dim)\n",
        "        self.dropout = config.dropout\n",
        "        self.activation_fn = nn.ReLU()\n",
        "        self.PositionWiseFeedForward = PositionWiseFeedForward(self.emb_dim, self.ffn_dim, config.dropout)\n",
        "        self.final_layer_norm = nn.LayerNorm(self.emb_dim)\n",
        "\n",
        "    def forward(self, x, encoder_padding_mask):\n",
        "\n",
        "        residual = x\n",
        "        x, attn_weights = self.self_attn(query=x, key=x, attention_mask=encoder_padding_mask)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = residual + x\n",
        "        x = self.self_attn_layer_norm(x)\n",
        "        x = self.PositionWiseFeedForward(x)\n",
        "        x = self.final_layer_norm(x)\n",
        "        if torch.isinf(x).any() or torch.isnan(x).any():\n",
        "            clamp_value = torch.finfo(x.dtype).max - 1000\n",
        "            x = torch.clamp(x, min=-clamp_value, max=clamp_value)\n",
        "        return x, attn_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LygNGzM0EbN8"
      },
      "source": [
        "### b. Encoder\n",
        "\n",
        "Stack encoder layers and build full Transformer encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZOAlAv7EbN8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, config, embed_tokens):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dropout = config.dropout\n",
        "\n",
        "        emb_dim = embed_tokens.embedding_dim\n",
        "        self.padding_idx = embed_tokens.padding_idx\n",
        "        self.max_source_positions = config.max_position_embeddings\n",
        "\n",
        "        self.embed_tokens = embed_tokens\n",
        "        self.embed_positions = SinusoidalPositionalEmbedding(\n",
        "                config.max_position_embeddings, config.emb_dim, self.padding_idx\n",
        "            )\n",
        "\n",
        "        self.layers = nn.ModuleList([EncoderLayer(config) for _ in range(config.encoder_layers)])\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "\n",
        "        inputs_embeds = self.embed_tokens(input_ids)\n",
        "        embed_pos = self.embed_positions(input_ids)\n",
        "        x = inputs_embeds + embed_pos\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        self_attn_scores = []\n",
        "        for encoder_layer in self.layers:\n",
        "            x, attn = encoder_layer(x, attention_mask)\n",
        "            self_attn_scores.append(attn.detach())\n",
        "\n",
        "        return x, self_attn_scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgjqDJnKEbN8"
      },
      "source": [
        "## 3. Transformer Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73LEB0mBEbN8"
      },
      "source": [
        "### a.Decoder layer\n",
        "In this section, we will implement single layer of Transformer decoder.\n",
        "![picture](http://incredible.ai/assets/images/transformer-decoder.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HgMu2QCEbN8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.emb_dim = config.emb_dim\n",
        "        self.ffn_dim = config.ffn_dim\n",
        "        self.self_attn = MultiHeadAttention(\n",
        "            emb_dim=self.emb_dim,\n",
        "            num_heads=config.attention_heads,\n",
        "            dropout=config.attention_dropout,\n",
        "            causal=True,\n",
        "        )\n",
        "        self.dropout = config.dropout\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(self.emb_dim)\n",
        "        self.encoder_attn = MultiHeadAttention(\n",
        "            emb_dim=self.emb_dim,\n",
        "            num_heads=config.attention_heads,\n",
        "            dropout=config.attention_dropout,\n",
        "            encoder_decoder_attention=True,\n",
        "        )\n",
        "        self.encoder_attn_layer_norm = nn.LayerNorm(self.emb_dim)\n",
        "        self.PositionWiseFeedForward = PositionWiseFeedForward(self.emb_dim, self.ffn_dim, config.dropout)\n",
        "        self.final_layer_norm = nn.LayerNorm(self.emb_dim)\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x,\n",
        "        encoder_hidden_states,\n",
        "        encoder_attention_mask=None,\n",
        "        causal_mask=None,\n",
        "    ):\n",
        "        residual = x\n",
        "        # Self Attention\n",
        "        x, self_attn_weights = self.self_attn(\n",
        "            query=x,\n",
        "            key=x, # adds keys to layer state\n",
        "            attention_mask=causal_mask,\n",
        "        )\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = residual + x\n",
        "        x = self.self_attn_layer_norm(x)\n",
        "\n",
        "        # Cross-Attention Block\n",
        "        residual = x\n",
        "        x, cross_attn_weights = self.encoder_attn(\n",
        "            query=x,\n",
        "            key=encoder_hidden_states,\n",
        "            attention_mask=encoder_attention_mask,\n",
        "        )\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = residual + x\n",
        "        x = self.encoder_attn_layer_norm(x)\n",
        "\n",
        "        # Fully Connected\n",
        "        x = self.PositionWiseFeedForward(x)\n",
        "        x = self.final_layer_norm(x)\n",
        "\n",
        "        return (\n",
        "            x,\n",
        "            self_attn_weights,\n",
        "            cross_attn_weights,\n",
        "        ) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAJQ-q5mEbN8"
      },
      "source": [
        "### b. Decoder\n",
        "\n",
        "Stack decoder layers and build full Transformer decoder.\n",
        "\n",
        "Unlike the encoder, you need to do one more job: pass the causal(unidirectional) mask to the decoder self attention layer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEMa6owhEbN8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, config, embed_tokens: nn.Embedding):\n",
        "        super().__init__()\n",
        "        self.dropout = config.dropout\n",
        "        self.padding_idx = embed_tokens.padding_idx\n",
        "        self.max_target_positions = config.max_position_embeddings\n",
        "        self.embed_tokens = embed_tokens\n",
        "        self.embed_positions = SinusoidalPositionalEmbedding(\n",
        "            config.max_position_embeddings, config.emb_dim, self.padding_idx\n",
        "        )\n",
        "        self.layers = nn.ModuleList([DecoderLayer(config) for _ in range(config.decoder_layers)])  # type: List[DecoderLayer]\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids,\n",
        "        encoder_hidden_states,\n",
        "        encoder_attention_mask,\n",
        "        decoder_causal_mask,\n",
        "    ):\n",
        "\n",
        "        # embed positions\n",
        "        positions = self.embed_positions(input_ids)\n",
        "        x = self.embed_tokens(input_ids) \n",
        "        x += positions\n",
        "\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        # decoder layers\n",
        "        cross_attention_scores = []\n",
        "        for idx, decoder_layer in enumerate(self.layers):\n",
        "            x, layer_self_attn, layer_cross_attn = decoder_layer(\n",
        "                x,\n",
        "                encoder_hidden_states,\n",
        "                encoder_attention_mask=encoder_attention_mask,\n",
        "                causal_mask=decoder_causal_mask,\n",
        "            )\n",
        "            cross_attention_scores.append(layer_cross_attn.detach())\n",
        "\n",
        "        return x, cross_attention_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr0g3oeIEbN8"
      },
      "source": [
        "## 4. Transformer\n",
        "\n",
        "Let's combine encoder and decoder in one place!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4aZzq8GEbN8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, SRC,TRG,config):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.SRC = SRC\n",
        "        self.TRG = TRG\n",
        "        \n",
        "        self.enc_embedding = nn.Embedding(len(SRC.vocab), config.emb_dim, padding_idx=SRC.vocab.stoi['<pad>'])\n",
        "        self.dec_embedding = nn.Embedding(len(TRG.vocab), config.emb_dim, padding_idx=TRG.vocab.stoi['<pad>'])\n",
        "\n",
        "        self.encoder = Encoder(config, self.enc_embedding)\n",
        "        self.decoder = Decoder(config, self.dec_embedding)\n",
        "        \n",
        "        self.prediction_head = nn.Linear(config.emb_dim,len(TRG.vocab))\n",
        "        \n",
        "        self.init_weights()\n",
        "        \n",
        "    def generate_mask(self,src,trg):\n",
        "        # Mask encoder attention to ignore padding\n",
        "        enc_attention_mask = src.eq(SRC.vocab.stoi['<pad>']).to(device)\n",
        "        # Mask decoder attention for causality\n",
        "        tmp = torch.ones(trg.size(1), trg.size(1),dtype=torch.bool)\n",
        "        mask = torch.arange(tmp.size(-1))\n",
        "        dec_attention_mask = tmp.masked_fill_(mask < (mask + 1).view(tmp.size(-1), 1), False).to(device)\n",
        "        \n",
        "        return enc_attention_mask, dec_attention_mask\n",
        "        \n",
        "    def init_weights(self):\n",
        "        for name, param in self.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                if 'weight' in name:\n",
        "                    nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "                else:\n",
        "                    nn.init.constant_(param.data, 0)\n",
        "        \n",
        "    def forward(\n",
        "        self,\n",
        "        src,\n",
        "        trg,\n",
        "    ):\n",
        "        enc_attention_mask, dec_causal_mask = self.generate_mask(src, trg)\n",
        "        encoder_output, encoder_attention_scores = self.encoder(\n",
        "                input_ids=src,\n",
        "                attention_mask=enc_attention_mask\n",
        "            )\n",
        "\n",
        "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
        "        decoder_output, decoder_attention_scores = self.decoder(\n",
        "            trg,\n",
        "            encoder_output,\n",
        "            encoder_attention_mask=enc_attention_mask,\n",
        "            decoder_causal_mask=dec_causal_mask,\n",
        "        )\n",
        "        decoder_output = self.prediction_head(decoder_output)\n",
        "\n",
        "        return decoder_output, encoder_attention_scores, decoder_attention_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WU-llE39EbN8"
      },
      "source": [
        "# III. Train & Evaluate\n",
        "<a id='3'></a>\n",
        "This section is very similar to week 9, so please refer to it for detailed description."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZRMlUmxEbN8"
      },
      "source": [
        "## 1. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlIc_VKaEbN8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import easydict\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "config = easydict.EasyDict({\n",
        "    \"emb_dim\":64,\n",
        "    \"ffn_dim\":256,\n",
        "    \"attention_heads\":4,\n",
        "    \"attention_dropout\":0.0,\n",
        "    \"dropout\":0.2,\n",
        "    \"max_position_embeddings\":512,\n",
        "    \"encoder_layers\":3,\n",
        "    \"decoder_layers\":3,\n",
        "    \n",
        "})\n",
        "\n",
        "N_EPOCHS = 100\n",
        "learning_rate = 5e-4\n",
        "CLIP = 1\n",
        "PAD_IDX = TRG.vocab.stoi['<pad>']\n",
        "\n",
        "model = Transformer(SRC,TRG,config)\n",
        "model.to(device)\n",
        "optimizer = optim.Adam(model.parameters(),lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "            \n",
        "best_valid_loss = float('inf')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hql5wOKEbN8"
      },
      "source": [
        "## 2. Train & Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1HHCxXuEbN8",
        "outputId": "9ad5bf1c-fff5-401b-f58b-bf140f76bc50",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def train(model: nn.Module,\n",
        "          iterator: BucketIterator,\n",
        "          optimizer: optim.Optimizer,\n",
        "          criterion: nn.Module,\n",
        "          clip: float):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for idx, batch in enumerate(iterator):\n",
        "\n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output, enc_attention_scores, _ = model(src, trg)\n",
        "\n",
        "        output = output[:,:-1,:].reshape(-1, output.shape[-1])\n",
        "        trg = trg[:,1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "\n",
        "def evaluate(model: nn.Module,\n",
        "             iterator: BucketIterator,\n",
        "             criterion: nn.Module):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for _, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output, attention_score, _ = model(src, trg) #turn off teacher forcing\n",
        "\n",
        "            output = output[:,:-1,:].reshape(-1, output.shape[-1])\n",
        "            trg = trg[:,1:].reshape(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "for epoch in tqdm(range(N_EPOCHS), total=N_EPOCHS):\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    if best_valid_loss < valid_loss:\n",
        "        break\n",
        "    else:\n",
        "        best_valid_loss = valid_loss\n",
        "\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxyJad1WEbN8"
      },
      "source": [
        "# IV. Visualization\n",
        "<a id='4'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_Eop7pGEbN8"
      },
      "source": [
        "## 1. Positional embedding visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJKGr5JfEbN8",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Visualization\n",
        "fig, ax = plt.subplots(figsize=(15, 9))\n",
        "cax = ax.matshow(model.encoder.embed_positions.weight.data.cpu().numpy(), aspect='auto',cmap=plt.cm.YlOrRd)\n",
        "fig.colorbar(cax)\n",
        "ax.set_title('Positional Embedding Matrix', fontsize=18)\n",
        "ax.set_xlabel('Embedding Dimension', fontsize=14)\n",
        "ax.set_ylabel('Sequence Length', fontsize=14)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrLCVOWlEbN8"
      },
      "source": [
        "## 2. Attention visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azwmQfF-EbN8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from attentionviz import head_view\n",
        "\n",
        "BATCH_SIZE = 1\n",
        "train_iterator, _, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device)\n",
        "\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLCz7R73EbN8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "if not 'attentionviz' in sys.path:\n",
        "  sys.path += ['attentionviz']\n",
        "!pip install regex\n",
        "\n",
        "def call_html():\n",
        "  import IPython\n",
        "  display(IPython.core.display.HTML('''\n",
        "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
        "        <script>\n",
        "          requirejs.config({\n",
        "            paths: {\n",
        "              base: '/static/base',\n",
        "              \"d3\": \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.8/d3.min\",\n",
        "              jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min',\n",
        "            },\n",
        "          });\n",
        "        </script>\n",
        "        '''))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkV8XEM2EbN9",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "SAMPLE_IDX = 131\n",
        "\n",
        "with torch.no_grad():\n",
        "  for idx,example in enumerate(test_iterator):\n",
        "    if idx == SAMPLE_IDX:\n",
        "      sample = example\n",
        "  src = sample.src\n",
        "  trg = sample.trg\n",
        "\n",
        "  output, enc_attention_score, dec_attention_score = model(src, trg) #turn off teacher forcing\n",
        "  attention_score = {'self':enc_attention_score, 'cross':dec_attention_score}\n",
        "\n",
        "  src_tok = [SRC.vocab.itos[x] for x in src.squeeze()]\n",
        "  trg_tok = [TRG.vocab.itos[x] for x in trg.squeeze()]\n",
        "\n",
        "  call_html()\n",
        "  head_view(attention_score, src_tok, trg_tok)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ai504_week11_transformer_sol.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
